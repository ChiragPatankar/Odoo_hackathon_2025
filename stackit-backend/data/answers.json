[
  {
    "id": "a1",
    "questionId": "q1",
    "content": "You can use the `mssql` package for Node.js. Here's a simple example:\n\n```javascript\nconst sql = require('mssql');\n\nconst config = {\n  user: 'your_username',\n  password: 'your_password',\n  server: 'your_server',\n  database: 'your_database'\n};\n\nsql.connect(config)\n  .then(pool => {\n    return pool.request().query('SELECT * FROM users');\n  })\n  .then(result => {\n    console.log(result);\n  });\n```\n\nMake sure to handle errors and use connection pooling for better performance.",
    "userId": "user1",
    "username": "admin",
    "votes": 13,
    "isAccepted": false,
    "createdAt": "2025-07-11T05:51:06.545Z"
  },
  {
    "id": "a2",
    "questionId": "q1",
    "content": "Another approach is to use TypeORM which provides better type safety:\n\n```typescript\nimport { createConnection } from 'typeorm';\n\nconst connection = await createConnection({\n  type: 'mssql',\n  host: 'localhost',\n  port: 1433,\n  username: 'your_username',\n  password: 'your_password',\n  database: 'your_database',\n  entities: [User, Question]\n});\n```",
    "userId": "user2",
    "username": "johndoe",
    "votes": 3,
    "isAccepted": false,
    "createdAt": "2025-07-11T06:51:06.545Z"
  },
  {
    "id": "de256e42-683d-44a0-8665-bacd77e958f7",
    "questionId": "q2",
    "content": "YOU R VERY RIGHT!!!!",
    "userId": "526a470a-d2db-4ca4-8d33-e1c829aad92d",
    "username": "chirag12",
    "votes": 0,
    "isAccepted": false,
    "createdAt": "2025-07-12T06:12:07.484Z"
  },
  {
    "id": "6b72b15a-207c-4cc1-905f-368ec89141cd",
    "questionId": "q2",
    "content": "Uncaught runtime errors:\nÃ—\nERROR\nObjects are not valid as a React child (found: object with keys {id, questionId, content, userId, username, votes, isAccepted, createdAt}). If you meant to render a collection of children, use an array instead.\n    at throwOnInvalidObjectType (http://localhost:3000/static/js/bundle.js:7183:11)\n    at createChild (http://localhost:3000/static/js/bundle.js:7276:9)\n    at reconcileChildrenArray (http://localhost:3000/static/js/bundle.js:7376:66)\n    at reconcileChildFibersImpl (http://localhost:3000/static/js/bundle.js:7483:109)\n    at http://localhost:3000/static/js/bundle.js:7510:31\n    at reconcileChildren (http://localhost:3000/static/js/bundle.js:7772:115)\n    at beginWork (http://localhost:3000/static/js/bundle.js:8537:16)\n    at runWithFiberInDEV (http://localhost:3000/static/js/bundle.js:3953:68)\n    at performUnitOfWork (http://localhost:3000/static/js/bundle.js:10534:93)\n    at workLoopSync (http://localhost:3000/static/js/bundle.js:10427:38)",
    "userId": "526a470a-d2db-4ca4-8d33-e1c829aad92d",
    "username": "chirag12",
    "votes": 1,
    "isAccepted": false,
    "createdAt": "2025-07-12T06:18:15.901Z"
  },
  {
    "id": "a3",
    "questionId": "q3",
    "content": "Here's a comprehensive approach to implementing authentication in React:\n\n## 1. JWT Token Management\n\n```javascript\n// auth.js\nconst TOKEN_KEY = 'authToken';\n\nexport const authService = {\n  login: async (credentials) => {\n    const response = await fetch('/api/auth/login', {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify(credentials)\n    });\n    \n    if (response.ok) {\n      const { token } = await response.json();\n      localStorage.setItem(TOKEN_KEY, token);\n      return token;\n    }\n    throw new Error('Login failed');\n  },\n  \n  logout: () => {\n    localStorage.removeItem(TOKEN_KEY);\n  },\n  \n  getToken: () => localStorage.getItem(TOKEN_KEY),\n  \n  isAuthenticated: () => !!localStorage.getItem(TOKEN_KEY)\n};\n```\n\n## 2. Protected Route Component\n\n```javascript\n// ProtectedRoute.js\nimport { Navigate } from 'react-router-dom';\nimport { authService } from './auth';\n\nconst ProtectedRoute = ({ children }) => {\n  return authService.isAuthenticated() ? children : <Navigate to=\"/login\" replace />;\n};\n\nexport default ProtectedRoute;\n```\n\n## 3. Auth Context Provider\n\n```javascript\n// AuthContext.js\nimport { createContext, useContext, useState, useEffect } from 'react';\nimport { authService } from './auth';\n\nconst AuthContext = createContext();\n\nexport const useAuth = () => {\n  const context = useContext(AuthContext);\n  if (!context) {\n    throw new Error('useAuth must be used within an AuthProvider');\n  }\n  return context;\n};\n\nexport const AuthProvider = ({ children }) => {\n  const [user, setUser] = useState(null);\n  const [loading, setLoading] = useState(true);\n\n  useEffect(() => {\n    const token = authService.getToken();\n    if (token) {\n      // Validate token and fetch user data\n      fetchUserData(token);\n    } else {\n      setLoading(false);\n    }\n  }, []);\n\n  const fetchUserData = async (token) => {\n    try {\n      const response = await fetch('/api/user/me', {\n        headers: { Authorization: `Bearer ${token}` }\n      });\n      \n      if (response.ok) {\n        const userData = await response.json();\n        setUser(userData);\n      } else {\n        authService.logout();\n      }\n    } catch (error) {\n      console.error('Error fetching user data:', error);\n      authService.logout();\n    } finally {\n      setLoading(false);\n    }\n  };\n\n  const login = async (credentials) => {\n    try {\n      const token = await authService.login(credentials);\n      await fetchUserData(token);\n      return { success: true };\n    } catch (error) {\n      return { success: false, error: error.message };\n    }\n  };\n\n  const logout = () => {\n    authService.logout();\n    setUser(null);\n  };\n\n  const value = {\n    user,\n    login,\n    logout,\n    loading,\n    isAuthenticated: !!user\n  };\n\n  return <AuthContext.Provider value={value}>{children}</AuthContext.Provider>;\n};\n```\n\n## 4. App Setup\n\n```javascript\n// App.js\nimport { BrowserRouter, Routes, Route } from 'react-router-dom';\nimport { AuthProvider } from './AuthContext';\nimport ProtectedRoute from './ProtectedRoute';\nimport Login from './Login';\nimport Dashboard from './Dashboard';\n\nfunction App() {\n  return (\n    <AuthProvider>\n      <BrowserRouter>\n        <Routes>\n          <Route path=\"/login\" element={<Login />} />\n          <Route \n            path=\"/dashboard\" \n            element={\n              <ProtectedRoute>\n                <Dashboard />\n              </ProtectedRoute>\n            } \n          />\n        </Routes>\n      </BrowserRouter>\n    </AuthProvider>\n  );\n}\n\nexport default App;\n```\n\n## Key Best Practices:\n\n1. **Token Storage**: Use localStorage for persistence, but consider httpOnly cookies for better security\n2. **Token Validation**: Always validate tokens on protected routes\n3. **Error Handling**: Implement proper error boundaries and fallbacks\n4. **Auto-logout**: Handle token expiration gracefully\n5. **HTTPS**: Always use HTTPS in production\n6. **CSRF Protection**: Implement CSRF tokens for additional security\n\nThis approach provides a robust authentication system with proper separation of concerns and good user experience.",
    "userId": "user1",
    "username": "auth_expert",
    "votes": 42,
    "isAccepted": true,
    "createdAt": "2025-01-15T11:15:00Z"
  },
  {
    "id": "a4",
    "questionId": "q4",
    "content": "Great question! Here's a comprehensive comparison of Python lists vs tuples:\n\n## Performance Comparison\n\n### Memory Usage\n```python\nimport sys\n\n# Lists use more memory due to dynamic resizing\nmy_list = [1, 2, 3, 4, 5]\nmy_tuple = (1, 2, 3, 4, 5)\n\nprint(f\"List size: {sys.getsizeof(my_list)} bytes\")\nprint(f\"Tuple size: {sys.getsizeof(my_tuple)} bytes\")\n# Output: List size: 104 bytes, Tuple size: 80 bytes\n```\n\n### Speed Comparison\n```python\nimport timeit\n\n# Creation speed\nlist_time = timeit.timeit(lambda: [1, 2, 3, 4, 5], number=1000000)\ntuple_time = timeit.timeit(lambda: (1, 2, 3, 4, 5), number=1000000)\n\nprint(f\"List creation: {list_time:.4f}s\")\nprint(f\"Tuple creation: {tuple_time:.4f}s\")\n# Tuples are ~2x faster to create\n\n# Access speed\nmy_list = [1, 2, 3, 4, 5] * 1000\nmy_tuple = tuple(my_list)\n\nlist_access = timeit.timeit(lambda: my_list[2500], number=1000000)\ntuple_access = timeit.timeit(lambda: my_tuple[2500], number=1000000)\n\nprint(f\"List access: {list_access:.4f}s\")\nprint(f\"Tuple access: {tuple_access:.4f}s\")\n# Access speeds are similar\n```\n\n## Key Differences\n\n| Feature | List | Tuple |\n|---------|------|-------|\n| **Mutability** | Mutable | Immutable |\n| **Memory** | More (dynamic) | Less (fixed) |\n| **Speed** | Slower creation | Faster creation |\n| **Methods** | Many (append, remove, etc.) | Few (count, index) |\n| **Use as dict key** | No | Yes |\n| **Syntax** | [1, 2, 3] | (1, 2, 3) |\n\n## When to Use Each\n\n### Use Lists When:\n- You need to modify the collection\n- The size varies during runtime\n- You need list methods (append, remove, etc.)\n\n```python\n# Good use case for lists\nshopping_cart = []\nshopping_cart.append(\"apples\")\nshopping_cart.append(\"bananas\")\nshopping_cart.remove(\"apples\")\n```\n\n### Use Tuples When:\n- Data shouldn't change (coordinates, RGB values)\n- You need hashable objects (dict keys)\n- You want better performance for large datasets\n- Returning multiple values from functions\n\n```python\n# Good use cases for tuples\ncoordinates = (10, 20)  # Point coordinates\nrgb_color = (255, 128, 0)  # Color values\n\n# As dictionary keys\nlocations = {\n    (0, 0): \"origin\",\n    (10, 20): \"point A\"\n}\n\n# Function return values\ndef get_name_age():\n    return \"John\", 25  # Returns a tuple\n\nname, age = get_name_age()  # Tuple unpacking\n```\n\n## Performance Tips\n\n1. **Use tuples for fixed data**: If your data won't change, tuples are more efficient\n2. **Convert lists to tuples**: When you're done modifying, convert for better performance\n3. **Named tuples**: For structured data with named fields\n\n```python\nfrom collections import namedtuple\n\n# More readable than regular tuples\nPoint = namedtuple('Point', ['x', 'y'])\npoint = Point(10, 20)\nprint(point.x, point.y)  # Access by name\n```\n\n## Benchmark Results\nFor 1 million operations:\n- Tuple creation: ~40% faster\n- Memory usage: ~20% less\n- Access speed: Nearly identical\n\n**Bottom line**: Use tuples for immutable data and when performance matters. Use lists when you need to modify the collection.",
    "userId": "user2",
    "username": "python_performance",
    "votes": 35,
    "isAccepted": true,
    "createdAt": "2025-01-14T15:30:00Z"
  },
  {
    "id": "a5",
    "questionId": "q5",
    "content": "Here's a comprehensive guide to Docker multi-stage builds for optimization:\n\n## Basic Multi-Stage Build Structure\n\n```dockerfile\n# Multi-stage Dockerfile example\n# Stage 1: Build stage\nFROM node:18-alpine AS builder\n\nWORKDIR /app\n\n# Copy package files\nCOPY package*.json ./\n\n# Install dependencies (including dev dependencies)\nRUN npm ci --only=production && npm cache clean --force\n\n# Copy source code\nCOPY . .\n\n# Build the application\nRUN npm run build\n\n# Stage 2: Production stage\nFROM node:18-alpine AS production\n\nWORKDIR /app\n\n# Create non-root user\nRUN addgroup -g 1001 -S nodejs\nRUN adduser -S nextjs -u 1001\n\n# Copy only necessary files from builder stage\nCOPY --from=builder /app/dist ./dist\nCOPY --from=builder /app/node_modules ./node_modules\nCOPY --from=builder /app/package*.json ./\n\n# Switch to non-root user\nUSER nextjs\n\nEXPOSE 3000\n\nCMD [\"npm\", \"start\"]\n```\n\n## Advanced Multi-Stage Example\n\n```dockerfile\n# Complex multi-stage build with multiple optimization layers\n\n# Stage 1: Dependencies\nFROM node:18-alpine AS deps\nRUN apk add --no-cache libc6-compat\nWORKDIR /app\n\n# Install dependencies based on the preferred package manager\nCOPY package.json yarn.lock* package-lock.json* pnpm-lock.yaml* ./\nRUN \\\n  if [ -f yarn.lock ]; then yarn --frozen-lockfile; \\\n  elif [ -f package-lock.json ]; then npm ci; \\\n  elif [ -f pnpm-lock.yaml ]; then yarn global add pnpm && pnpm i; \\\n  else echo \"Lockfile not found.\" && exit 1; \\\n  fi\n\n# Stage 2: Builder\nFROM node:18-alpine AS builder\nWORKDIR /app\nCOPY --from=deps /app/node_modules ./node_modules\nCOPY . .\n\n# Build application\nRUN npm run build\n\n# Stage 3: Production runner\nFROM node:18-alpine AS runner\nWORKDIR /app\n\nENV NODE_ENV production\n\nRUN addgroup --system --gid 1001 nodejs\nRUN adduser --system --uid 1001 nextjs\n\n# Copy built application\nCOPY --from=builder /app/dist ./dist\nCOPY --from=builder /app/public ./public\n\n# Automatically leverage output traces to reduce image size\nCOPY --from=builder --chown=nextjs:nodejs /app/.next/standalone ./\nCOPY --from=builder --chown=nextjs:nodejs /app/.next/static ./.next/static\n\nUSER nextjs\n\nEXPOSE 3000\n\nENV PORT 3000\n\nCMD [\"node\", \"server.js\"]\n```\n\n## Best Practices\n\n### 1. Order Layers by Change Frequency\n```dockerfile\n# Do this - stable layers first\nFROM node:18-alpine\nWORKDIR /app\n\n# Package files change less frequently\nCOPY package*.json ./\nRUN npm ci --only=production\n\n# Source code changes more frequently\nCOPY . .\nRUN npm run build\n```\n\n### 2. Use .dockerignore\n```dockerignore\n# .dockerignore\nnode_modules\n.git\n.gitignore\nREADME.md\n.env\n.nyc_output\ncoverage\n.coverage\n.coverage.*\n*.log\n```\n\n### 3. Minimize Layer Size\n```dockerfile\n# Combine RUN commands\nRUN apt-get update && apt-get install -y \\\n    curl \\\n    git \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Use multi-line for readability\nRUN apt-get update \\\n    && apt-get install -y \\\n        curl \\\n        git \\\n    && rm -rf /var/lib/apt/lists/*\n```\n\n### 4. Use Specific Base Images\n```dockerfile\n# Use specific versions and slim variants\nFROM node:18.17.0-alpine3.18 AS base\n\n# Use distroless for minimal attack surface\nFROM gcr.io/distroless/nodejs18-debian11 AS runner\n```\n\n## Performance Optimization Tips\n\n### 1. Parallel Builds\n```dockerfile\n# Build frontend and backend in parallel\nFROM node:18-alpine AS frontend-builder\nWORKDIR /app/frontend\nCOPY frontend/package*.json ./\nRUN npm ci\nCOPY frontend/ .\nRUN npm run build\n\nFROM node:18-alpine AS backend-builder\nWORKDIR /app/backend\nCOPY backend/package*.json ./\nRUN npm ci\nCOPY backend/ .\nRUN npm run build\n\n# Final stage combines both\nFROM node:18-alpine AS runner\nCOPY --from=frontend-builder /app/frontend/dist ./public\nCOPY --from=backend-builder /app/backend/dist ./\n```\n\n### 2. Build Cache Optimization\n```dockerfile\n# Use BuildKit for better caching\n# syntax=docker/dockerfile:1\nFROM node:18-alpine AS base\n\n# Mount cache for npm\nRUN --mount=type=cache,target=/root/.npm \\\n    npm ci --only=production\n\n# Mount cache for build artifacts\nRUN --mount=type=cache,target=/app/.cache \\\n    npm run build\n```\n\n### 3. Build Arguments for Environment\n```dockerfile\nARG NODE_ENV=production\nARG BUILD_VERSION=latest\n\nENV NODE_ENV=${NODE_ENV}\nENV BUILD_VERSION=${BUILD_VERSION}\n\n# Conditional logic based on build args\nRUN if [ \"$NODE_ENV\" = \"production\" ]; then \\\n        npm ci --only=production; \\\n    else \\\n        npm ci; \\\n    fi\n```\n\n## Size Reduction Techniques\n\n### Before and After Comparison\n```bash\n# Before multi-stage (single stage)\nIMAGE SIZE: 1.2GB\n\n# After multi-stage optimization\nIMAGE SIZE: 85MB\n\n# 93% size reduction!\n```\n\n### Build and Analyze\n```bash\n# Build with BuildKit\nDOCKER_BUILDKIT=1 docker build -t myapp:optimized .\n\n# Analyze image layers\ndocker history myapp:optimized\n\n# Use dive for detailed analysis\ndive myapp:optimized\n```\n\n## Complete Example: Go Application\n\n```dockerfile\n# Multi-stage build for Go application\nFROM golang:1.21-alpine AS builder\n\n# Install git and ca-certificates\nRUN apk add --no-cache git ca-certificates\n\n# Create appuser\nRUN adduser -D -g '' appuser\n\nWORKDIR /build\n\n# Copy go mod files\nCOPY go.mod go.sum ./\nRUN go mod download\n\n# Copy source code\nCOPY . .\n\n# Build the binary\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -ldflags '-extldflags \"-static\"' -o main .\n\n# Final stage\nFROM scratch\n\n# Copy ca-certificates from builder\nCOPY --from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/\n\n# Copy user from builder\nCOPY --from=builder /etc/passwd /etc/passwd\n\n# Copy binary from builder\nCOPY --from=builder /build/main /app/\n\n# Use unprivileged user\nUSER appuser\n\nEXPOSE 8080\n\nENTRYPOINT [\"/app/main\"]\n```\n\n## Key Benefits\n\n1. **Size Reduction**: 50-95% smaller images\n2. **Security**: Fewer attack surfaces\n3. **Performance**: Faster pulls and deployments\n4. **Separation**: Clear build vs runtime environments\n5. **Caching**: Better layer caching strategies\n\nThis approach dramatically reduces image size while maintaining all necessary runtime dependencies!",
    "userId": "user3",
    "username": "docker_expert",
    "votes": 38,
    "isAccepted": true,
    "createdAt": "2025-01-13T10:20:00Z"
  },
  {
    "id": "a6",
    "questionId": "q6",
    "content": "Excellent question! Let me break down async/await vs Promises with clear examples:\n\n## Understanding Promises First\n\n```javascript\n// Traditional Promise syntax\nfunction fetchUserData(userId) {\n  return fetch(`/api/users/${userId}`)\n    .then(response => {\n      if (!response.ok) {\n        throw new Error('User not found');\n      }\n      return response.json();\n    })\n    .then(userData => {\n      console.log('User data:', userData);\n      return userData;\n    })\n    .catch(error => {\n      console.error('Error:', error);\n      throw error;\n    });\n}\n\n// Using the Promise\nfetchUserData(123)\n  .then(user => {\n    console.log('Got user:', user.name);\n  })\n  .catch(error => {\n    console.error('Failed to get user:', error);\n  });\n```\n\n## The Same Code with Async/Await\n\n```javascript\n// Async/await syntax - much cleaner!\nasync function fetchUserData(userId) {\n  try {\n    const response = await fetch(`/api/users/${userId}`);\n    \n    if (!response.ok) {\n      throw new Error('User not found');\n    }\n    \n    const userData = await response.json();\n    console.log('User data:', userData);\n    return userData;\n  } catch (error) {\n    console.error('Error:', error);\n    throw error;\n  }\n}\n\n// Using the async function\nasync function main() {\n  try {\n    const user = await fetchUserData(123);\n    console.log('Got user:', user.name);\n  } catch (error) {\n    console.error('Failed to get user:', error);\n  }\n}\n\nmain();\n```\n\n## Key Differences\n\n| Feature | Promises | Async/Await |\n|---------|----------|-------------|\n| **Syntax** | .then().catch() | try/catch |\n| **Readability** | Can get nested | Linear, like sync code |\n| **Error Handling** | .catch() chains | Standard try/catch |\n| **Debugging** | Harder to debug | Easier to debug |\n| **Multiple Operations** | Promise.all() | await Promise.all() |\n\n## Complex Example: Multiple API Calls\n\n### With Promises (callback hell)\n```javascript\n// This gets messy quickly!\nfunction getUserProfile(userId) {\n  return fetch(`/api/users/${userId}`)\n    .then(response => response.json())\n    .then(user => {\n      return fetch(`/api/posts/${user.id}`)\n        .then(response => response.json())\n        .then(posts => {\n          return fetch(`/api/comments/${posts[0].id}`)\n            .then(response => response.json())\n            .then(comments => {\n              return {\n                user,\n                posts,\n                comments\n              };\n            });\n        });\n    });\n}\n```\n\n### With Async/Await (clean and readable)\n```javascript\n// Much cleaner and easier to understand!\nasync function getUserProfile(userId) {\n  try {\n    // Sequential execution\n    const userResponse = await fetch(`/api/users/${userId}`);\n    const user = await userResponse.json();\n    \n    const postsResponse = await fetch(`/api/posts/${user.id}`);\n    const posts = await postsResponse.json();\n    \n    const commentsResponse = await fetch(`/api/comments/${posts[0].id}`);\n    const comments = await commentsResponse.json();\n    \n    return {\n      user,\n      posts,\n      comments\n    };\n  } catch (error) {\n    console.error('Error fetching profile:', error);\n    throw error;\n  }\n}\n```\n\n## Parallel vs Sequential Execution\n\n### Sequential (one after another)\n```javascript\nasync function getDataSequentially() {\n  const user = await fetchUser(1);     // Wait for this\n  const posts = await fetchPosts(1);   // Then wait for this\n  const comments = await fetchComments(1); // Then wait for this\n  \n  return { user, posts, comments };\n}\n// Total time: ~300ms (100ms each)\n```\n\n### Parallel (all at once)\n```javascript\nasync function getDataInParallel() {\n  // Start all requests simultaneously\n  const [user, posts, comments] = await Promise.all([\n    fetchUser(1),\n    fetchPosts(1),\n    fetchComments(1)\n  ]);\n  \n  return { user, posts, comments };\n}\n// Total time: ~100ms (all run together)\n```\n\n## Error Handling Patterns\n\n### Individual Error Handling\n```javascript\nasync function robustDataFetching() {\n  const results = {};\n  \n  // Handle each request individually\n  try {\n    results.user = await fetchUser(1);\n  } catch (error) {\n    console.error('User fetch failed:', error);\n    results.user = null;\n  }\n  \n  try {\n    results.posts = await fetchPosts(1);\n  } catch (error) {\n    console.error('Posts fetch failed:', error);\n    results.posts = [];\n  }\n  \n  return results;\n}\n```\n\n### Promise.allSettled for Partial Success\n```javascript\nasync function fetchAllData() {\n  const results = await Promise.allSettled([\n    fetchUser(1),\n    fetchPosts(1),\n    fetchComments(1)\n  ]);\n  \n  return results.map((result, index) => {\n    if (result.status === 'fulfilled') {\n      return result.value;\n    } else {\n      console.error(`Request ${index} failed:`, result.reason);\n      return null;\n    }\n  });\n}\n```\n\n## Common Pitfalls and Solutions\n\n### 1. Forgetting to await\n```javascript\n// Wrong - returns a Promise, not the value!\nasync function wrong() {\n  const data = fetchData(); // Missing await!\n  console.log(data); // Promise<pending>\n  return data;\n}\n\n// Correct\nasync function correct() {\n  const data = await fetchData();\n  console.log(data); // Actual data\n  return data;\n}\n```\n\n### 2. Using await in loops incorrectly\n```javascript\n// Slow - sequential execution\nfor (const id of userIds) {\n  const user = await fetchUser(id);\n  users.push(user);\n}\n\n// Fast - parallel execution\nconst users = await Promise.all(\n  userIds.map(id => fetchUser(id))\n);\n```\n\n### 3. Not handling errors\n```javascript\n// Dangerous - unhandled promise rejection\nasync function dangerous() {\n  const data = await fetchData(); // Could throw\n  return data;\n}\n\n// Safe - proper error handling\nasync function safe() {\n  try {\n    const data = await fetchData();\n    return data;\n  } catch (error) {\n    console.error('Error:', error);\n    throw error; // Re-throw or handle appropriately\n  }\n}\n```\n\n## Best Practices\n\n1. **Use async/await for readability** - it's easier to understand than Promise chains\n2. **Handle errors properly** - always use try/catch\n3. **Use Promise.all for parallel operations** - don't await in loops unnecessarily\n4. **Return early** - avoid deep nesting even with async/await\n5. **Use Promise.allSettled** - when you want to handle partial failures\n\n## Quick Summary\n\n- **Promises** are the foundation - async/await is syntactic sugar\n- **Async/await** makes asynchronous code look synchronous\n- **Both are interoperable** - async functions return Promises\n- **Use async/await** for cleaner, more maintainable code\n- **Understand both** - you'll encounter both in real codebases\n\nThe key insight: `async/await` doesn't replace Promises - it makes them easier to work with!",
    "userId": "user4",
    "username": "js_async_expert",
    "votes": 56,
    "isAccepted": true,
    "createdAt": "2025-01-12T17:45:00Z"
  },
  {
    "id": "a7",
    "questionId": "q7",
    "content": "Great question! MongoDB aggregation optimization is crucial for performance. Here's a comprehensive guide:\n\n## Understanding the Aggregation Pipeline\n\n```javascript\n// Basic aggregation structure\ndb.collection.aggregate([\n  { $match: { status: 'active' } },    // Filter early\n  { $sort: { createdAt: -1 } },        // Sort\n  { $group: { _id: '$category', count: { $sum: 1 } } },\n  { $limit: 10 }                       // Limit results\n])\n```\n\n## Key Optimization Principles\n\n### 1. Filter Early with $match\n```javascript\n// Good - filter early\ndb.orders.aggregate([\n  { $match: { \n    date: { $gte: new Date('2024-01-01') },\n    status: 'completed' \n  }},\n  { $group: { _id: '$product', total: { $sum: '$amount' } } }\n])\n\n// Bad - filter late\ndb.orders.aggregate([\n  { $group: { _id: '$product', total: { $sum: '$amount' } } },\n  { $match: { total: { $gt: 1000 } } }  // Should be earlier if possible\n])\n```\n\n### 2. Use Indexes Effectively\n```javascript\n// Create compound indexes for your queries\ndb.orders.createIndex({ date: 1, status: 1, product: 1 })\n\n// Query that uses the index\ndb.orders.aggregate([\n  { $match: { \n    date: { $gte: new Date('2024-01-01') },\n    status: 'completed'\n  }},\n  { $group: { _id: '$product', total: { $sum: '$amount' } } }\n])\n```\n\n### 3. Optimize $lookup Operations\n```javascript\n// Inefficient - no index on foreign field\ndb.orders.aggregate([\n  {\n    $lookup: {\n      from: 'products',\n      localField: 'productId',\n      foreignField: '_id',  // Make sure this is indexed!\n      as: 'product'\n    }\n  }\n])\n\n// Better - with proper indexing\ndb.products.createIndex({ _id: 1 })  // Usually exists by default\n\n// Even better - filter before lookup\ndb.orders.aggregate([\n  { $match: { status: 'completed' } },  // Filter first\n  {\n    $lookup: {\n      from: 'products',\n      localField: 'productId',\n      foreignField: '_id',\n      as: 'product'\n    }\n  }\n])\n```\n\n## Advanced Optimization Techniques\n\n### 1. Pipeline Optimization Order\n```javascript\n// Optimal pipeline order\ndb.sales.aggregate([\n  // 1. Filter early (uses indexes)\n  { $match: { \n    date: { $gte: new Date('2024-01-01') },\n    amount: { $gt: 100 }\n  }},\n  \n  // 2. Sort early (can use indexes)\n  { $sort: { date: -1 } },\n  \n  // 3. Limit early to reduce document processing\n  { $limit: 1000 },\n  \n  // 4. Project to reduce document size\n  { $project: {\n    date: 1,\n    amount: 1,\n    product: 1\n  }},\n  \n  // 5. Expensive operations last\n  { $lookup: { /* ... */ } },\n  { $unwind: '$product' }\n])\n```\n\n### 2. Memory-Efficient Grouping\n```javascript\n// For large datasets, use $group with $addToSet carefully\n// Bad - can exceed memory limits\ndb.orders.aggregate([\n  { $group: {\n    _id: '$customer',\n    allProducts: { $addToSet: '$product' }  // Can be huge!\n  }}\n])\n\n// Better - use $push with $slice\ndb.orders.aggregate([\n  { $group: {\n    _id: '$customer',\n    recentProducts: { $push: '$product' }\n  }},\n  { $project: {\n    _id: 1,\n    recentProducts: { $slice: ['$recentProducts', 10] }  // Limit size\n  }}\n])\n```\n\n### 3. Optimize Text Search\n```javascript\n// Create text index\ndb.products.createIndex({ name: 'text', description: 'text' })\n\n// Use $text for full-text search\ndb.products.aggregate([\n  { $match: { \n    $text: { $search: 'laptop computer' },\n    price: { $lt: 1000 }\n  }},\n  { $sort: { score: { $meta: 'textScore' } } }\n])\n```\n\n## Performance Monitoring\n\n### 1. Use explain() to analyze performance\n```javascript\n// Get execution stats\ndb.orders.explain('executionStats').aggregate([\n  { $match: { status: 'completed' } },\n  { $group: { _id: '$product', count: { $sum: 1 } } }\n])\n\n// Look for:\n// - totalDocsExamined vs totalDocsReturned\n// - executionTimeMillis\n// - indexesUsed\n```\n\n### 2. Monitor with profiler\n```javascript\n// Enable profiling for slow operations\ndb.setProfilingLevel(2, { slowms: 100 })\n\n// Check profiler collection\ndb.system.profile.find().sort({ ts: -1 }).limit(5)\n```\n\n## Specific Optimization Patterns\n\n### 1. Large Dataset Aggregation\n```javascript\n// Use allowDiskUse for large operations\ndb.orders.aggregate([\n  { $match: { date: { $gte: new Date('2024-01-01') } } },\n  { $group: { _id: '$product', total: { $sum: '$amount' } } },\n  { $sort: { total: -1 } }\n], { allowDiskUse: true })\n```\n\n### 2. Real-time Analytics\n```javascript\n// Use $merge for incremental updates\ndb.orders.aggregate([\n  { $match: { processedAt: { $exists: false } } },\n  { $group: {\n    _id: { product: '$product', date: '$date' },\n    count: { $sum: 1 },\n    total: { $sum: '$amount' }\n  }},\n  { $merge: {\n    into: 'daily_stats',\n    on: '_id',\n    whenMatched: 'merge',\n    whenNotMatched: 'insert'\n  }}\n])\n```\n\n### 3. Geospatial Aggregation\n```javascript\n// Create geospatial index\ndb.stores.createIndex({ location: '2dsphere' })\n\n// Efficient geospatial aggregation\ndb.stores.aggregate([\n  { $geoNear: {\n    near: { type: 'Point', coordinates: [-73.9857, 40.7484] },\n    distanceField: 'distance',\n    maxDistance: 1000,\n    spherical: true\n  }},\n  { $group: {\n    _id: '$category',\n    count: { $sum: 1 },\n    avgDistance: { $avg: '$distance' }\n  }}\n])\n```\n\n## Common Performance Pitfalls\n\n### 1. Avoid unnecessary $unwind\n```javascript\n// Bad - $unwind creates many documents\ndb.orders.aggregate([\n  { $unwind: '$items' },\n  { $group: { _id: '$_id', itemCount: { $sum: 1 } } }\n])\n\n// Good - use $size instead\ndb.orders.aggregate([\n  { $project: {\n    itemCount: { $size: '$items' }\n  }}\n])\n```\n\n### 2. Be careful with $facet\n```javascript\n// $facet runs all sub-pipelines on all documents\ndb.orders.aggregate([\n  { $match: { status: 'completed' } },  // Filter first!\n  { $facet: {\n    totalSales: [{ $group: { _id: null, total: { $sum: '$amount' } } }],\n    topProducts: [{ $group: { _id: '$product', count: { $sum: 1 } } }]\n  }}\n])\n```\n\n## Index Strategies\n\n```javascript\n// Compound indexes for aggregation\ndb.orders.createIndex({ \n  status: 1,     // Equality match\n  date: -1,      // Range/sort\n  amount: 1      // Range\n})\n\n// Partial indexes for specific queries\ndb.orders.createIndex(\n  { customerId: 1, date: -1 },\n  { partialFilterExpression: { status: 'completed' } }\n)\n\n// Text indexes for search aggregations\ndb.products.createIndex(\n  { name: 'text', description: 'text' },\n  { weights: { name: 10, description: 5 } }\n)\n```\n\n## Performance Checklist\n\nâœ… **Filter early** with $match\nâœ… **Use indexes** for $match, $sort, $group\nâœ… **Limit documents** with $limit\nâœ… **Project only needed fields**\nâœ… **Use $lookup efficiently** with proper indexes\nâœ… **Monitor with explain()** and profiler\nâœ… **Consider $merge** for incremental updates\nâœ… **Use allowDiskUse** for large datasets\nâœ… **Avoid unnecessary $unwind**\nâœ… **Test with realistic data volumes**\n\n## Real-world Example\n\n```javascript\n// Optimized e-commerce analytics query\ndb.orders.aggregate([\n  // 1. Filter early with indexed fields\n  { $match: {\n    createdAt: { $gte: new Date('2024-01-01') },\n    status: 'completed'\n  }},\n  \n  // 2. Project early to reduce document size\n  { $project: {\n    customerId: 1,\n    amount: 1,\n    items: 1,\n    createdAt: 1\n  }},\n  \n  // 3. Efficient grouping\n  { $group: {\n    _id: {\n      customer: '$customerId',\n      month: { $dateToString: { format: '%Y-%m', date: '$createdAt' } }\n    },\n    orderCount: { $sum: 1 },\n    totalAmount: { $sum: '$amount' },\n    avgAmount: { $avg: '$amount' }\n  }},\n  \n  // 4. Sort by computed field\n  { $sort: { totalAmount: -1 } },\n  \n  // 5. Limit results\n  { $limit: 100 }\n], { allowDiskUse: true })\n```\n\nWith these optimizations, you should see significant performance improvements on large datasets!",
    "userId": "user5",
    "username": "mongo_performance",
    "votes": 41,
    "isAccepted": false,
    "createdAt": "2025-01-11T13:10:00Z"
  },
  {
    "id": "a8",
    "questionId": "q8",
    "content": "Excellent question! Understanding when to use `git merge` vs `git rebase` is crucial for maintaining a clean project history. Here's a comprehensive breakdown:\n\n## Git Merge\n\n### How it works\n```bash\n# Create a merge commit that combines two branches\ngit checkout main\ngit merge feature-branch\n\n# Creates a merge commit with two parents\n# History looks like:\n#   * Merge branch 'feature-branch' into main\n#  /|\n# * | Feature commit 2\n# * | Feature commit 1\n# |/\n# * Previous main commit\n```\n\n### Pros of Merge\n- **Preserves complete history** - shows exactly what happened\n- **Non-destructive** - doesn't change existing commits\n- **Clear branch context** - easy to see when features were integrated\n- **Safer for shared branches** - won't rewrite public history\n- **Handles conflicts once** - conflicts resolved in merge commit\n\n### Cons of Merge\n- **Cluttered history** - many merge commits can be noisy\n- **Complex graph** - harder to follow linear progression\n- **Harder to revert** - reverting a merge affects multiple commits\n\n## Git Rebase\n\n### How it works\n```bash\n# Replay commits from feature branch onto main\ngit checkout feature-branch\ngit rebase main\n\n# Creates new commits with same changes but different parents\n# History looks like:\n# * Feature commit 2 (new)\n# * Feature commit 1 (new)\n# * Latest main commit\n# * Previous main commit\n```\n\n### Pros of Rebase\n- **Clean linear history** - easier to follow and understand\n- **Better for code review** - cleaner commit log\n- **Easier to use git tools** - bisect, log, etc. work better\n- **No merge commits** - eliminates merge commit noise\n- **Easy to revert** - can revert individual commits\n\n### Cons of Rebase\n- **Rewrites history** - changes commit SHAs\n- **Dangerous on shared branches** - can cause conflicts for teammates\n- **Conflict resolution** - may need to resolve same conflict multiple times\n- **Loses merge context** - harder to see when features were integrated\n\n## When to Use Each\n\n### Use Merge When:\n\n1. **Working on public/shared branches**\n```bash\n# Main branch - use merge to avoid rewriting public history\ngit checkout main\ngit merge feature-branch\n```\n\n2. **Want to preserve feature branch context**\n```bash\n# Keep the history of when feature was integrated\ngit merge --no-ff feature-branch\n```\n\n3. **Working with team members on same branch**\n```bash\n# Safer for collaboration\ngit pull origin main  # This is actually a merge\n```\n\n### Use Rebase When:\n\n1. **Working on personal feature branches**\n```bash\n# Clean up your commits before merging\ngit rebase main\ngit checkout main\ngit merge feature-branch  # This will be a fast-forward\n```\n\n2. **Want to clean up commit history**\n```bash\n# Interactive rebase to squash/edit commits\ngit rebase -i HEAD~3\n```\n\n3. **Integrating latest changes while developing**\n```bash\n# Keep your feature branch up to date\ngit checkout feature-branch\ngit rebase main\n```\n\n## Practical Workflow Examples\n\n### Scenario 1: Feature Development\n```bash\n# Start feature\ngit checkout -b feature-login\ngit commit -m \"Add login form\"\ngit commit -m \"Add validation\"\ngit commit -m \"Fix typo\"\n\n# Main branch has been updated\n# Option 1: Merge (preserves branching)\ngit checkout main\ngit merge feature-login\n\n# Option 2: Rebase (clean history)\ngit checkout feature-login\ngit rebase main\ngit checkout main\ngit merge feature-login  # Fast-forward merge\n```\n\n### Scenario 2: Collaborative Feature\n```bash\n# Multiple developers working on same feature\n# Use merge to avoid conflicts\n\n# Developer 1\ngit checkout feature-api\ngit commit -m \"Add user endpoint\"\ngit push origin feature-api\n\n# Developer 2\ngit checkout feature-api\ngit pull origin feature-api  # This is a merge\ngit commit -m \"Add auth middleware\"\ngit push origin feature-api\n```\n\n### Scenario 3: Hotfix\n```bash\n# Critical bug fix\ngit checkout -b hotfix-security\ngit commit -m \"Fix security vulnerability\"\n\n# Fast integration - use merge\ngit checkout main\ngit merge hotfix-security\ngit tag v1.2.1\n```\n\n## Interactive Rebase (Powerful Tool)\n\n```bash\n# Clean up commits before merging\ngit rebase -i HEAD~3\n\n# Interactive editor opens:\n# pick f7f3f6d Add login form\n# squash 310154e Add validation  \n# squash a5f4a0d Fix typo\n\n# Options:\n# pick = use commit\n# squash = combine with previous\n# edit = modify commit\n# drop = remove commit\n# reword = change commit message\n```\n\n## Team Workflow Strategies\n\n### Strategy 1: Merge-Only\n```bash\n# Simple, safe approach\ngit checkout main\ngit merge feature-branch\ngit push origin main\n\n# Pros: Simple, safe, preserves history\n# Cons: Cluttered history\n```\n\n### Strategy 2: Rebase-Then-Merge\n```bash\n# Clean history approach\ngit checkout feature-branch\ngit rebase main          # Clean up\ngit checkout main\ngit merge feature-branch # Fast-forward\ngit push origin main\n\n# Pros: Clean history, easy to follow\n# Cons: More complex workflow\n```\n\n### Strategy 3: Squash-Merge\n```bash\n# GitHub's approach\ngit checkout main\ngit merge --squash feature-branch\ngit commit -m \"Add login feature\"\ngit push origin main\n\n# Pros: One commit per feature\n# Cons: Loses individual commit history\n```\n\n## Best Practices\n\n### Golden Rules\n1. **Never rebase public/shared branches**\n2. **Use merge for public integration**\n3. **Use rebase for local cleanup**\n4. **Be consistent within your team**\n5. **Test after rebasing**\n\n### Safe Rebase Workflow\n```bash\n# Always work on a copy\ngit checkout feature-branch\ngit checkout -b feature-branch-backup\ngit checkout feature-branch\ngit rebase main\n\n# If something goes wrong:\ngit reset --hard feature-branch-backup\n```\n\n### Handling Conflicts\n\n**Merge conflicts (once per merge):**\n```bash\ngit merge feature-branch\n# Fix conflicts in files\ngit add .\ngit commit  # Completes merge\n```\n\n**Rebase conflicts (once per commit):**\n```bash\ngit rebase main\n# Fix conflicts in files\ngit add .\ngit rebase --continue  # Continue with next commit\n```\n\n## Quick Decision Guide\n\n**Use MERGE when:**\n- âœ… Working on shared/public branches\n- âœ… Want to preserve branch context\n- âœ… Working with team on same branch\n- âœ… Integrating major features\n- âœ… Want to be safe and simple\n\n**Use REBASE when:**\n- âœ… Working on personal feature branches\n- âœ… Want clean linear history\n- âœ… Preparing code for review\n- âœ… Updating feature branch with latest main\n- âœ… Team agrees on rebase workflow\n\n## Common Mistakes to Avoid\n\n1. **Rebasing public branches** - causes conflicts for team\n2. **Not testing after rebase** - conflicts might break functionality\n3. **Rebasing without backup** - hard to recover from mistakes\n4. **Inconsistent team workflow** - mixing strategies confuses everyone\n5. **Force pushing without care** - can overwrite team members' work\n\n## Summary\n\n- **Merge** preserves history and is safer for collaboration\n- **Rebase** creates cleaner history but requires more care\n- **Use merge** for public branches and team collaboration\n- **Use rebase** for personal branches and history cleanup\n- **Be consistent** within your team's workflow\n- **When in doubt, use merge** - it's safer\n\nThe key is understanding your team's workflow and being consistent!",
    "userId": "user6",
    "username": "git_workflow_expert",
    "votes": 68,
    "isAccepted": true,
    "createdAt": "2025-01-10T09:30:00Z"
  }
]